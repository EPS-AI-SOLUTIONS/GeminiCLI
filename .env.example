# =============================================================================
# GeminiHydra - Environment Variables
# Copy this file to .env and fill in the values.
# =============================================================================

# -----------------------------------------------------------------------------
# REQUIRED
# -----------------------------------------------------------------------------

# Google Gemini API key (get one at https://aistudio.google.com/apikey)
GEMINI_API_KEY=your-gemini-api-key-here

# -----------------------------------------------------------------------------
# PROVIDER / MODEL SETTINGS (optional)
# -----------------------------------------------------------------------------

# Override the default Gemini model used for chat completions
# Default: gemini-2.0-flash
# HYDRA_MODEL=gemini-2.0-flash

# Override the model used by MultiModalSupport and other direct GenAI calls
# Default: gemini-3-flash-preview
# GEMINI_MODEL=gemini-3-flash-preview

# URL of a local LLM server (switches provider to 'local' when set)
# Default: http://localhost:8000
# LOCAL_LLM_URL=http://localhost:8000
# -----------------------------------------------------------------------------
# SWARM SETTINGS (optional)
# -----------------------------------------------------------------------------

# Maximum number of parallel swarm tasks
# Default: 3
# HYDRA_MAX_TASKS=3

# Timeout for swarm task execution in milliseconds
# Default: 60000
# HYDRA_TIMEOUT=60000

# -----------------------------------------------------------------------------
# FEATURE FLAGS (optional)
# -----------------------------------------------------------------------------

# Run in headless mode (suppresses non-error log output)
# Default: false
# HYDRA_HEADLESS=false

# Enable verbose logging (shows agent thinking, debug details)
# Default: false
# HYDRA_VERBOSE=false

# Enable response streaming
# Default: true (set to "false" to disable)
# HYDRA_STREAMING=true

# Show verbose startup diagnostics
# Default: false
# VERBOSE_STARTUP=false
# Disable the built-in prompt injection detection
# Default: false (injection detection is ON)
# DISABLE_PROMPT_INJECTION_DETECTION=false

# Enable MCP (Model Context Protocol) adapter
# Default: false
# MCP_ENABLED=false

# General debug flag (enables extra diagnostics in NativeShell and others)
# Default: unset
# DEBUG=false

# Debug output specifically for the native shell subsystem
# Default: unset
# NATIVE_SHELL_DEBUG=false

# Node environment (affects Fastify pretty-printing, cookie security, etc.)
# Default: development
# NODE_ENV=development

# -----------------------------------------------------------------------------
# LLAMA.CPP PROVIDER (optional - for LlamaCppProvider)
# -----------------------------------------------------------------------------

# Base URL of the llama.cpp OpenAI-compatible server
# Default: http://localhost:8081
# LLAMA_CPP_URL=http://localhost:8081

# Model name to request from the llama.cpp server
# Default: default
# LLAMA_CPP_MODEL=default
# API key for the llama.cpp server (if authentication is enabled)
# Default: unset (no auth)
# LLAMA_CPP_API_KEY=

# Request timeout in milliseconds for llama.cpp calls
# Default: 120000
# LLAMA_CPP_TIMEOUT=120000

# -----------------------------------------------------------------------------
# LLAMA.CPP SERVER MANAGEMENT (optional - for LlamaCppServer)
# -----------------------------------------------------------------------------

# Directory containing .gguf model files
# Default: auto-detected (~/.cache/llama-cpp/models or ~/models)
# LLAMA_CPP_MODELS_DIR=

# Explicit path to a single .gguf model file
# Default: unset (auto-detected from LLAMA_CPP_MODELS_DIR)
# LLAMA_CPP_MODEL_PATH=

# Port for the managed llama.cpp server
# Default: 8000
# LLAMA_CPP_PORT=8000

# Context size (number of tokens) for llama.cpp
# Default: 4096
# LLAMA_CPP_CTX=4096

# Number of GPU layers to offload (-1 = all layers)
# Default: -1
# LLAMA_CPP_GPU_LAYERS=-1

# Chat template format (e.g. "llama-3", "chatml")
# Default: unset (auto-detected by llama.cpp)
# LLAMA_CPP_CHAT_FORMAT=

# Enable verbose output from the llama.cpp server process
# Default: false
# LLAMA_CPP_VERBOSE=false